{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import socket\n",
    "import struct\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import sys\n",
    "import random\n",
    "import psutil\n",
    "random.seed(0)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.initial_seed(), \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if CUDA is available, else use CPU\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "USERS = 1\n",
    "CUT_LAYER = 3\n",
    "BOTTLENECK_COMPRESSION = 4\n",
    "QUANTIZATION_REQUIRED = 1 # 1 for required\n",
    "QUANTIZATION_TYPE = torch.qint8 # np.uint8, np.int8, np.uint16, np.int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_msg(sock, msg):\n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    msg = pickle.dumps(msg)\n",
    "    msg = struct.pack('>I', len(msg)) + msg\n",
    "    sock.sendall(msg)\n",
    "    return len(msg)\n",
    "\n",
    "def recv_msg(sock):\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msglen = recvall(sock, 4)\n",
    "    if not raw_msglen:\n",
    "        return None\n",
    "    msglen = struct.unpack('>I', raw_msglen)[0]\n",
    "    # read the message data\n",
    "    msg =  recvall(sock, msglen)\n",
    "    msg = pickle.loads(msg)\n",
    "    return msg, msglen\n",
    "\n",
    "def recvall(sock, n):\n",
    "    # helper function to receive n MB or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'localhost'\n",
    "port = 12313\n",
    "\n",
    "s = socket.socket()\n",
    "s.bind((host, port))\n",
    "s.listen(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientsoclist = []\n",
    "train_total_batch = []\n",
    "val_acc = []\n",
    "\n",
    "total_sendsize_list = []\n",
    "total_receivesize_list = []\n",
    "\n",
    "client_sendsize_list = [[] for i in range(USERS)]\n",
    "client_receivesize_list = [[] for i in range(USERS)]\n",
    "\n",
    "train_sendsize_list = [] \n",
    "train_receivesize_list = []\n",
    "\n",
    "send_time_list = [[] for i in range(USERS)]\n",
    "recv_time_list = [[] for i in range(USERS)]\n",
    "total_time_list = [[] for i in range(USERS)]\n",
    "\n",
    "send_bandwidth_list = [[] for i in range(USERS)]\n",
    "recv_bandwidth_list = [[] for i in range(USERS)]\n",
    "total_bandwidth_list = [[] for i in range(USERS)]\n",
    "\n",
    "for i in range(USERS):\n",
    "    conn, addr = s.accept()\n",
    "    print(\"conn: \", conn)\n",
    "    print('Conntected with', addr)\n",
    "    clientsoclist.append(conn)    # append client socket on list\n",
    "    msg = {\n",
    "            'epochs': EPOCHS,\n",
    "            'users': USERS,\n",
    "            'cut_layer': CUT_LAYER,\n",
    "            'bottleneck_compression': BOTTLENECK_COMPRESSION,\n",
    "            'quantization_required': QUANTIZATION_REQUIRED,\n",
    "            'QUANTIZATION_TYPE': QUANTIZATION_TYPE\n",
    "    }\n",
    "    datasize = send_msg(conn, msg)    #send epoch\n",
    "    total_sendsize_list.append(sys.getsizeof(msg))\n",
    "    client_sendsize_list[i].append(sys.getsizeof(msg))\n",
    "\n",
    "    total_batch, datasize = recv_msg(conn)    # get total_batch of train dataset\n",
    "    print(\"Total batch {}\", total_batch)\n",
    "    total_receivesize_list.append(sys.getsizeof(total_batch))\n",
    "    client_receivesize_list[i].append(sys.getsizeof(total_batch))\n",
    "\n",
    "    train_total_batch.append(total_batch)    # append on list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClientResNet(nn.Module):\n",
    "    def __init__(self, block, output_channel=[64,128,256,512], stride=[1,2,2,2], num_blocks=[1,1,1,1], num_classes=10, \n",
    "                 bottleneck_compression=4, cut_layer=1):\n",
    "        super(ClientResNet, self).__init__()\n",
    "        \n",
    "        self.in_channels = 64\n",
    "        self.output_channel = output_channel\n",
    "        self.stride = stride\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_classes = num_classes\n",
    "        self.bottleneck_compression = bottleneck_compression\n",
    "        self.cut_layer = cut_layer\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layers = []\n",
    "        for layer in range(0, self.cut_layer):\n",
    "            self.layers.append(self._make_layer(block, self.output_channel[layer], self.num_blocks[layer], stride=self.stride[layer]))\n",
    "        self.conv_layers = nn.Sequential(*self.layers)\n",
    "\n",
    "        if self.bottleneck_compression>=1:\n",
    "            self.bl_encoder = nn.Conv2d(output_channel[self.cut_layer-1], output_channel[self.cut_layer-1]//bottleneck_compression, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.maxpool(out)\n",
    "        out = self.conv_layers(out)\n",
    "        if self.bottleneck_compression>=1:\n",
    "            out = self.bl_encoder(out)\n",
    "        return out\n",
    "\n",
    "# Create the model\n",
    "client_model = ClientResNet(block=BasicBlock, bottleneck_compression=BOTTLENECK_COMPRESSION, cut_layer=CUT_LAYER)\n",
    "client_model = client_model.to(device)\n",
    "print(client_model)\n",
    "#print(client_model.conv_layers[0][0].conv1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServerResNet(nn.Module):\n",
    "    def __init__(self, block, output_channel=[64,128,256,512], stride=[1,2,2,2], num_blocks=[1,1,1,1], num_classes=10, \n",
    "                 bottleneck_compression=4, cut_layer=1):\n",
    "        super(ServerResNet, self).__init__()\n",
    "\n",
    "        self.output_channel = output_channel\n",
    "        self.stride = stride\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_classes = num_classes\n",
    "        self.bottleneck_compression = bottleneck_compression\n",
    "        self.cut_layer = cut_layer\n",
    "        self.in_channels = self.output_channel[self.cut_layer-1]\n",
    "        \n",
    "\n",
    "        if self.bottleneck_compression>=1:\n",
    "            self.bl_decoder = nn.Conv2d(self.output_channel[self.cut_layer-1]//self.bottleneck_compression, self.output_channel[self.cut_layer-1], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    \n",
    "        self.layers = []\n",
    "        for layer in range(self.cut_layer, 4):\n",
    "            self.layers.append(self._make_layer(block, self.output_channel[layer], self.num_blocks[layer], stride=self.stride[layer]))\n",
    "        self.conv_layers = nn.Sequential(*self.layers)\n",
    " \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, self.num_classes)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        if self.bottleneck_compression>=1:\n",
    "            x = self.bl_decoder(x)\n",
    "        out = self.conv_layers(x)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Create the model\n",
    "server_model = ServerResNet(block=BasicBlock, output_channel=[64,128,256,512], stride=[1,2,2,2], num_blocks=[1,1,1,1], num_classes=10, \n",
    "                 bottleneck_compression=BOTTLENECK_COMPRESSION, cut_layer=CUT_LAYER)\n",
    "server_model = server_model.to(device)\n",
    "print(server_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(server_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_tensor(tensor, quantization_type):\n",
    "\n",
    "    min_val, max_val = tensor.min(), tensor.max()\n",
    "\n",
    "    if quantization_type == torch.quint8:\n",
    "        scale = torch.tensor((max_val - min_val) / 255)\n",
    "        zero_point = torch.tensor(max(0, min(255, int(-min_val / scale))))\n",
    "        return torch.quantize_per_tensor(tensor, scale, zero_point, torch.quint8)\n",
    "\n",
    "    elif quantization_type == torch.qint8:\n",
    "        scale = torch.tensor(max_val / 127 if max_val.abs() > min_val.abs() else min_val / -128)\n",
    "        zero_point = torch.tensor(0)  # For qint8, zero_point is typically set to 0 for symmetric quantization\n",
    "        return torch.quantize_per_tensor(tensor, scale, zero_point, torch.qint8)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid quantization type. Choose 'quint8' or 'qint8'.\")\n",
    "\n",
    "def dequantize_tensor(quantized_tensor):\n",
    "    return torch.dequantize(quantized_tensor).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_weights = copy.deepcopy(client_model.state_dict())\n",
    "\n",
    "start_time = time.time()    # store start time\n",
    "print(\"timer start!\")\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "\n",
    "    # train client 0\n",
    "    for user in range(USERS):\n",
    "\n",
    "        datasize = send_msg(clientsoclist[user], client_weights)\n",
    "        total_sendsize_list.append(datasize)\n",
    "        client_sendsize_list[user].append(datasize)\n",
    "        train_sendsize_list.append(datasize)\n",
    "\n",
    "        recv_time_total = 0\n",
    "        send_time_total = 0\n",
    "        total_time = 0\n",
    "\n",
    "        for i in tqdm(range(train_total_batch[user]), ncols=100, desc='Epoch {} Client{} '.format(e+1, user)):\n",
    "            optimizer.zero_grad()  # initialize all gradients to zero\n",
    "\n",
    "            recv_start_time = time.time()\n",
    "            msg, datasize = recv_msg(clientsoclist[user])  # receive client message from socket\n",
    "            recv_end_time = time.time()\n",
    "\n",
    "            recv_time = recv_end_time - recv_start_time\n",
    "            recv_time_total += recv_time\n",
    "            recv_time_avg = recv_time_total/(i+1)\n",
    "\n",
    "            total_receivesize_list.append(datasize)\n",
    "            client_receivesize_list[user].append(datasize)\n",
    "            train_receivesize_list.append(datasize)\n",
    "\n",
    "            client_output = msg['client_output']  # client output tensor\n",
    "            label = msg['label']  # label\n",
    "            label = label.clone().detach().long()\n",
    "            label = label.to(device)\n",
    "            \n",
    "            if QUANTIZATION_REQUIRED == 1:\n",
    "                client_output = dequantize_tensor(client_output)\n",
    "            \n",
    "            output = server_model(client_output).to(device)  # forward propagation\n",
    "            loss = criterion(output, label)  # calculates cross-entropy loss\n",
    "            loss.backward()  # backward propagation   \n",
    "                \n",
    "            if QUANTIZATION_REQUIRED == 1:\n",
    "                msg = quantize_tensor(client_output.grad.clone().detach().requires_grad_(True), QUANTIZATION_TYPE)\n",
    "            \n",
    "            else:\n",
    "                msg = client_output.grad.clone().detach().requires_grad_(True)\n",
    "            \n",
    "            send_start_time = time.time()\n",
    "            datasize = send_msg(clientsoclist[user], msg)\n",
    "            send_end_time = time.time()\n",
    "\n",
    "            send_time = send_end_time - send_start_time\n",
    "            send_time_total += send_time\n",
    "            send_time_avg = send_time_total/(i+1)\n",
    "\n",
    "            total_sendsize_list.append(datasize)\n",
    "            client_sendsize_list[user].append(datasize)\n",
    "            train_sendsize_list.append(datasize)\n",
    "\n",
    "            total_time += (send_time + recv_time)\n",
    "            total_time_avg = total_time/(i+1)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        client_weights, datasize = recv_msg(clientsoclist[user])\n",
    "        total_receivesize_list.append(datasize)\n",
    "        client_receivesize_list[user].append(datasize)\n",
    "        train_receivesize_list.append(datasize)\n",
    "        \n",
    "        send_time_list[user].append(send_time_avg)\n",
    "        recv_time_list[user].append(recv_time_avg)\n",
    "        total_time_list[user].append(total_time_avg)\n",
    "\n",
    "        \n",
    "        \n",
    "print('train is done')\n",
    "\n",
    "end_time = time.time()  # store end time\n",
    "print(\"TrainingTime: {} sec\".format(end_time - start_time))\n",
    "\n",
    "# Let's quickly save our trained model:\n",
    "PATH = 'server_3_q_b.pth'\n",
    "torch.save(server_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "client_model.load_state_dict(client_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    corr_num = 0\n",
    "    total_num = 0\n",
    "    val_loss = 0.0\n",
    "    for i, val in enumerate(tqdm(trainloader, desc=\"Training Data\")):\n",
    "        val_x, val_label = val\n",
    "        val_x = val_x.to(device)\n",
    "        val_label = val_label.to(device)\n",
    "\n",
    "        val_output = client_model(val_x)\n",
    "        val_label = val_label.long()\n",
    "        \n",
    "        val_output = server_model(val_output)\n",
    "        \n",
    "        loss = criterion(val_output, val_label)\n",
    "        val_loss += loss.item()\n",
    "        model_label = val_output.argmax(dim=1)\n",
    "        corr = val_label[val_label == model_label].size(0)\n",
    "        corr_num += corr\n",
    "        total_num += val_label.size(0)\n",
    "    print(\"train_acc: {:.2f}%, train_loss: {:.4f}\".format(corr_num / total_num * 100, val_loss / len(trainloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    corr_num = 0\n",
    "    total_num = 0\n",
    "    val_loss = 0.0\n",
    "    for i, val in enumerate(tqdm(testloader, desc=\"Testing Data\")):\n",
    "        val_x, val_label = val\n",
    "        val_x = val_x.to(device)\n",
    "        val_label = val_label.to(device)\n",
    "\n",
    "        val_output = client_model(val_x)\n",
    "        val_label = val_label.long()\n",
    "        \n",
    "        val_output = server_model(val_output)\n",
    "        \n",
    "        loss = criterion(val_output, val_label)\n",
    "        val_loss += loss.item()\n",
    "        model_label = val_output.argmax(dim=1)\n",
    "        corr = val_label[val_label == model_label].size(0)\n",
    "        corr_num += corr\n",
    "        total_num += val_label.size(0)\n",
    "    print(\"test_acc: {:.2f}%, test_loss: {:.4f}\".format(corr_num / total_num * 100, val_loss / len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(tqdm(testloader, desc=\"Testing Data\")):\n",
    "        x, labels = data\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = client_model(x)\n",
    "        outputs = server_model(outputs)\n",
    "        labels = labels.long()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import profile\n",
    "x =  torch.randn(50000, 3, 32, 32)\n",
    "x = x.to(device)\n",
    "macs, params = profile(client_model, inputs=(x,))\n",
    "print(macs, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import profile\n",
    "x =  torch.randn(client_output.shape)\n",
    "x = x.to(device)\n",
    "macs, params = profile(server_model, inputs=(x,))\n",
    "print(macs, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'path_to_file' with your file path\n",
    "file_path = 'server_3_q_b.pth'\n",
    "\n",
    "# Get the size of the file in bytes\n",
    "file_size = os.path.getsize(file_path)\n",
    "\n",
    "print(f\"The size of the file is {file_size / (1024**2)} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current Memory Usage: {psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_send_total_size_list = [[] for i in range(USERS)]\n",
    "client_recv_total_size_list = [[] for i in range(USERS)]\n",
    "\n",
    "print('---total_sendsize_list---')\n",
    "totalsend_size = 0\n",
    "for size in total_sendsize_list:\n",
    "    totalsend_size += size\n",
    "print(\"total_sendsize size: {} MB\".format(totalsend_size/(1024**2)))\n",
    "print(\"number of total_send: \", len(total_sendsize_list))\n",
    "print('\\n')\n",
    "\n",
    "print('---total_receivesize_list---')\n",
    "total_receive_size = 0\n",
    "for size in total_receivesize_list:\n",
    "    total_receive_size += size\n",
    "print(\"total receive sizes: {} MB\".format(total_receive_size/(1024**2)) )\n",
    "print(\"number of total receive: \", len(total_receivesize_list) )\n",
    "print('\\n')\n",
    "\n",
    "for i in range(USERS):\n",
    "    print('---client_sendsize_list(user{})---'.format(i))\n",
    "    total_size = 0\n",
    "    for size in client_sendsize_list[i]:\n",
    "#         print(size)\n",
    "        total_size += size\n",
    "    client_send_total_size_list[i] = total_size\n",
    "    print(\"total client_sendsizes(user{}): {} MB\".format(i, client_send_total_size_list[i]/(1024**2)))\n",
    "    print(\"number of client_send(user{}): \".format(i), len(client_sendsize_list[i]))\n",
    "    print('\\n')\n",
    "\n",
    "    print('---client_receivesize_list(user{})---'.format(i))\n",
    "    total_size = 0\n",
    "    for size in client_receivesize_list[i]:\n",
    "#         print(size)\n",
    "        total_size += size\n",
    "    client_recv_total_size_list[i] = total_size\n",
    "    print(\"total client_receive sizes(user{}): {} MB\".format(i, client_recv_total_size_list[i]/(1024**2)))\n",
    "    print(\"number of client_send(user{}): \".format(i), len(client_receivesize_list[i]))\n",
    "    print('\\n')\n",
    "\n",
    "print('---train_sendsize_list---')\n",
    "total_size = 0\n",
    "for size in train_sendsize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "train_sendsize = total_size\n",
    "print(\"total train_sendsizes: {} MB\".format(train_sendsize/(1024**2)))\n",
    "print(\"number of train_send: \", len(train_sendsize_list) )\n",
    "print('\\n')\n",
    "\n",
    "print('---train_receivesize_list---')\n",
    "total_size = 0\n",
    "for size in train_receivesize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "train_recvsize = total_size\n",
    "print(\"total train_receivesizes: {} MB\".format(train_recvsize/(1024**2)))\n",
    "print(\"number of train_receive: \", len(train_receivesize_list) )\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(USERS):\n",
    "    print('client_send_time(user{}) - {} seconds'.format(i, send_time_list[i]))\n",
    "\n",
    "for i in range(USERS):\n",
    "    print('client_receive_time(user{}) - {} seconds'.format(i, recv_time_list[i]))\n",
    "\n",
    "for i in range(USERS):\n",
    "    print('client_total_time(user{}) - {} seconds'.format(i, total_time_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(USERS):\n",
    "    print('client_send_bandwidth(user{}) - {} MB/sec'.format(i, (client_send_total_size_list[i]/send_time_list[i][0])/(1024**2)))\n",
    "\n",
    "for i in range(USERS):\n",
    "    print('client_receive_bandwidth(user{}) - {} MB/sec'.format(i, (client_recv_total_size_list[i]/recv_time_list[i][0])/(1024**2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
